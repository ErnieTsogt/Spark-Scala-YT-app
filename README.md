# **YouTube Comments Analysis with Apache Spark and MySQL**


This project is a Scala application that processes YouTube video metadata stored in a MySQL database using **Apache Spark**. It calculates daily comment statistics per video and saves the results both as a **CSV file** and into a **MySQL** database table.


## **Features**


- **Spark Data Processing**: Efficiently processes large datasets using Apache Spark.
- **Daily Comment Aggregation**: Aggregates the latest comments for each YouTube video on a daily basis.
- **Data Storage**: Saves the results as a CSV file and also inserts the data into a MySQL database table for persistence.
  
## **Prerequisites**


Before running this application, ensure you have the following set up:

- **Java 8+**
- **Scala**\
- **Apache Spark**\
- **MySQL** (you can run it locally or within a Docker container)\
- **SBT** (Scala Build Tool)
- **Docker** (to containerize the Spark app)

## **MySQL Database Setup**

You’ll need to create the necessary table in your MySQL database. Here is the SQL script for creating the `daily_comments` table:

```sql
CREATE TABLE IF NOT EXISTS ytScanDB.daily_comments (
   id              INT AUTO_INCREMENT PRIMARY KEY,
   google_vid_id   TEXT,
   total_comments  INT,
   report_date     DATETIME
);
```

The `ytvideos` table in the MySQL database should contain video metadata like `scanned_date`, `google_vid_id`, and `comments`.

## **How to Build and Run the Application**

### **Step 1: Build the Application with SBT**

To compile and package the application using **SBT**, run the following commands:

```bash
sbt clean compile package
```

This command will create a JAR file that can be used to run the Spark application.

### **Step 2: Running with Docker**

To run the application inside a **Docker container**:

1. **Build the Docker image**:

   ```bash\n" +
   docker build -t yt-spark-app .
   ```\n\n" +
2. **Run the Docker container** and map the output folder to your host:

   ```bash
   docker run -v $(pwd)/output:/app/output yt-spark-app
   ```
   
This command maps the `/app/output` folder from the container to an `output` folder on your host machine, making it easy to retrieve the CSV file generated by the app.

### **Step 3: Output**

The application outputs the results to:

- A **CSV file** stored in the `/app/output` directory of the container (or your mapped directory on the host machine).
- A **MySQL table** called `daily_comments` for persistence.
## **How the Program Works**

1. **Reading Data**: The application reads data from the `ytvideos` table in the MySQL database using Spark's `jdbc` reader.

2. **Processing**:
   - It converts the raw `scanned_date` into a proper timestamp using `from_unixtime()`.
   - Extracts the date (as `report_date`) and hour (as `comment_hour`) from the timestamp.
   - Uses a **window function** to partition the data by `google_vid_id` and `report_date` and ranks entries by timestamp to keep only the latest comment for each video per day.
   
3. **Storing Results**:
   - The processed data is written to a **CSV file** in the output folder.
   - The same data is appended to the **`daily_comments` table** in the MySQL database.
   


## **Troubleshooting**


- **File Permission Issues**: If you encounter file permission problems when writing to CSV, ensure the volume mapping is correct, and Docker has the appropriate permissions.

- **MySQL Connection Issues**: Double-check the database configuration (`jdbcUrl`, user credentials, etc.) to ensure the Spark app can connect to MySQL properly.


## **Contributing**

If you’d like to contribute to this project, feel free to open a **pull request** or **issue** with suggestions, improvements, or bug fixes.\n
