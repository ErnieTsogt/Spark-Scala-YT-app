YouTube Comments Analysis with Spark and MySQL

This project is a Spark-based Scala application that processes YouTube video data from a MySQL database, computes daily comment counts per video, and stores the results both as a CSV file and into a MySQL table.

Features

Data Processing: Extracts video metadata, calculates daily statistics, and filters for the latest comment data per day for each video.
Storage: Results are stored in both a CSV file and a MySQL database.
Scalable: Utilizes Apache Spark for distributed data processing.
Prerequisites

Before running the application, ensure you have the following installed:

Java 8+
Scala
Apache Spark
MySQL (configured in a Docker container or local environment)
SBT (Scala Build Tool)
Docker (for containerizing the app)

Building and Running the Application

Step 1: Build the Application
To build the project using SBT, run the following commands:

bash
Skopiuj kod
sbt clean compile package
This will compile the project and package it into a JAR file.

Step 2: Running with Docker
The application can be run in a Docker container. Follow these steps:

Build the Docker Image:
bash
Skopiuj kod
docker build -t yt-spark-app .
Run the Docker Container:
Use Docker to run the container, mapping the output CSV to your local filesystem:

bash
Skopiuj kod
docker run -v $(pwd)/output:/app/output yt-spark-app
This command maps the container's /app/output directory to the output folder on your host machine, allowing you to retrieve the CSV file generated by the application.
Step 3: Output
The application will store the results of the processed data in:
A CSV file: /app/output/daily_comments.csv (in the Docker container).
A MySQL table: daily_comments (in the MySQL database).
How the Program Works

Reading Data: The application reads video data from the ytvideos table in MySQL.
Processing:
scanned_timestamp: Converts raw scanned_date into a proper timestamp.
report_date: Extracts the date part from scanned_timestamp.
comment_hour: Extracts the hour of the day from scanned_timestamp.
Ranking and Filtering: Using Spark's Window functions, the application ranks comments by timestamp and filters to keep only the latest comment data for each video per day.
Storing Results:
The processed data is written into a CSV file in the /app/output folder.
The same data is appended to the daily_comments table in MySQL.
